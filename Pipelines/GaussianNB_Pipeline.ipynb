{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204b13ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: tqdm in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (4.66.4)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rpierson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation + '``'+ '`'+ ''+ ',' + '/')\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did not end up using preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7c2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Preprocessing subpipeline\n",
    "class PreprocessingPineline:\n",
    "    def __init__(self, stop_words, punctuation):\n",
    "        self.punctuation = punctuation\n",
    "        self.stop_words = stop_words\n",
    "        self.data = None\n",
    "        self.tokens = None        \n",
    "        \n",
    "    def data_to_tokens(self, data):\n",
    "        self.data = data\n",
    "        self.tokens = self.data.astype(str).apply(word_tokenize)\n",
    "        self.tokens = [[word for word in tokens if word.lower() not in stop_words and word not in punctuation] for tokens in self.tokens]\n",
    "        return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2acc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained usage of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d819cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Topic Modeling subPipeline\n",
    "class LDATopicModelPipeline:\n",
    "    def __init__(self, lda_model_path, vectorizer_path):\n",
    "        self.lda_model_path = lda_model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.lda = None\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.lda = joblib.load(self.lda_model_path)\n",
    "        self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        \n",
    "    def topic_distributions(self, new_documents):\n",
    "        if self.lda is None or self.vectorizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if isinstance(new_documents[\"Combined_Text\"], list):\n",
    "            # If new_documents is a list of strings\n",
    "            texts = new_documents\n",
    "        elif isinstance(new_documents[\"Combined_Text\"], pd.Series):\n",
    "            # If new_documents is a Pandas Series (assuming it's a single column from a DataFrame)\n",
    "            texts = new_documents[\"Combined_Text\"].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a list, Pandas Series, or DataFrame of strings.\")\n",
    "            \n",
    "        x = self.vectorizer.transform(texts)\n",
    "        topic_distributions = self.lda.transform(x)\n",
    "        return topic_distributions\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def append_topics(self, data, topic_distributions):\n",
    "        topics = []\n",
    "        for topic_dist in topic_distributions:\n",
    "            dominant_topic = topic_dist.argmax()\n",
    "            topics.append(dominant_topic)\n",
    "            \n",
    "        data['Topic'] = topics\n",
    "        return data\n",
    "            \n",
    "#gives necessary files to process information\n",
    "#use through this syntax: topic_distributions = topic_model.topic_distributions(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Text Classifiers used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c7cddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an Text Classification subpipeline\n",
    "class TextClassificationNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.documents = None\n",
    "        self.vectorizer = None\n",
    "        self.bayes_trained = None\n",
    "        \n",
    "        \n",
    "    def load_model(self, topic_num):\n",
    "        #Use Trained Text Classifier Based on Topic Number\n",
    "         for x in [topic_num]:\n",
    "            self.bayes_trained = joblib.load(f'/home/rpierson/githubPierson/TrainingNaiveBayesClassifiers/bayes/topic_pred_{x}.pkl')\n",
    "            self.vectorizer = joblib.load(f'/home/rpierson/githubPierson/TrainingNaiveBayesClassifiers/bayes/vec_{x}.pkl')\n",
    "    def priority(self, documents):\n",
    "        self.num_topics = documents['Topic'].drop_duplicates().values\n",
    "        documents[\"Predicted_Priority\"] = \"\" \n",
    "        self.documents = documents\n",
    "        \n",
    "        for topic_num in self.num_topics:\n",
    "            self.load_model(topic_num)\n",
    "            for i in self.documents.index: \n",
    "                if self.documents.loc[i, 'Topic'] == topic_num:\n",
    "                    text = self.documents.loc[i, \"Combined_Text\"]\n",
    "                    vector = self.vectorizer.transform([text]).toarray().reshape(1, -1)\n",
    "                    prediction = self.bayes_trained.predict(vector)\n",
    "                    self.documents.at[i, \"Predicted_Priority\"] = prediction[0]\n",
    "        return self.documents\n",
    "        \n",
    "    \n",
    "        #topic_prediction_function.predict(topic_validation_data.toarray())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy assessments for our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2df03af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AccuracyAssessment:\n",
    "    def __init__(self, priority_levels):\n",
    "        self.actual_priority = None\n",
    "        self.predicted_priority = None\n",
    "        self.true_pos = 0\n",
    "        self.false_pos = 0\n",
    "        self.false_neg = 0\n",
    "        self.num_priority_levels = priority_levels\n",
    "        self.confusion_matrix = np.zeros((self.num_priority_levels, self.num_priority_levels), dtype=int)\n",
    "    \n",
    "    def update_vals(self, actual, predicted):\n",
    "        self.actual_priority = actual.astype(int)\n",
    "        self.predicted_priority = predicted.astype(int)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        for i in range(len(self.actual_priority)):\n",
    "            true_idx = self.actual_priority[i] - 1\n",
    "            pred_idx = self.predicted_priority[i] - 1\n",
    "            self.confusion_matrix[true_idx, pred_idx] += 1\n",
    "    \n",
    "    def calc_metrics(self, class_index):\n",
    "        self.true_pos = self.confusion_matrix[class_index, class_index]\n",
    "        self.false_pos = np.sum(self.confusion_matrix[:, class_index]) - self.true_pos\n",
    "        self.false_neg = np.sum(self.confusion_matrix[class_index, :]) - self.true_pos\n",
    "    \n",
    "    def precision(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_pos == 0:\n",
    "            return 0\n",
    "        precision = self.true_pos / (self.true_pos + self.false_pos)\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    def recall(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_neg == 0:\n",
    "            return 0\n",
    "        \n",
    "        recall = self.true_pos / (self.true_pos + self.false_neg)\n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    def fmeasure(self, class_index):\n",
    "        precision = self.precision(class_index)\n",
    "        recall = self.recall(class_index)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        \n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        return fmeasure\n",
    "    \n",
    "    def accuracyOverall(self):\n",
    "        accuratePriority = 0\n",
    "        for i in range(self.num_priority_levels):\n",
    "            self.calc_metrics(i)\n",
    "            accuratePriority += self.true_pos\n",
    "        accuratePriorities = accuratePriority / len(self.actual_priority)\n",
    "        return accuratePriorities\n",
    "    \n",
    "    def microAnalysis(self):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f_measures = []\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "            \n",
    "            precisions.append(precision_score)\n",
    "            recalls.append(recall_score)\n",
    "            f_measures.append(f_measure_score)\n",
    "        \n",
    "        micro_precision = sum(precisions) / sum(precisions + recalls)\n",
    "        micro_recall = sum(precisions) / sum(precisions + f_measures)\n",
    "        micro_fmeasure = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "        print(f\"Micro-Analysis for Priority Levels: Precision = {micro_precision:.4f}, Recall={micro_recall:.4f}, F-measure={micro_fmeasure:.4f}\")\n",
    "   \n",
    "    def macroAnalysis(self):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f_measures = []\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "            \n",
    "            precisions.append(precision_score)\n",
    "            recalls.append(recall_score)\n",
    "            f_measures.append(f_measure_score)\n",
    "\n",
    "        macro_precision = sum(precisions) / len(precisions)\n",
    "        macro_recall = sum(recalls) / len(recalls)\n",
    "        macro_fmeasure = sum(f_measures) / len(f_measures)\n",
    "        print(f\"Macro-Analysis for Priority Levels: Precision = {macro_precision:.4f}, Recall={macro_recall:.4f}, F-measure={macro_fmeasure:.4f}\")\n",
    "    def printAssessment(self):\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "    \n",
    "            print(f\"Priority P{i+1}: Precision={precision_score:.4f}, Recall={recall_score:.4f}, F-measure={f_measure_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I prefer without assessment, this priority_pipeline may need adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc04cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline(data, actual):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "   # preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "   # data = preprocess.data_to_tokens(data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    data = nb.priority(df)\n",
    "    print(data[\"Predicted_Priority\"])\n",
    "    print(data.groupby('Predicted_Priority').sum())\n",
    "    \n",
    "    #Accuracy and Evaluation\n",
    "    assessment = AccuracyAssessment(priority_levels = 5)\n",
    "    assessment.update_vals(actual, data[\"Predicted_Priority\"])\n",
    "    assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9545193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to test our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d387007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline_without_assessment(data):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "    #preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "    #data = preprocess.data_to_tokens(data)\n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    df['Combined_Text'] = df['Combined_Text'].fillna(' ')\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    data = nb.priority(df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f5fd2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "to_see_results = pd.read_csv('/home/rpierson/PiersonREU/extracted/test_dataset_notpreprocessed.csv')\n",
    "to_see_results.head()\n",
    "\n",
    "to_see_results['Combined_Text'] = to_see_results['Title'] + \" \" + to_see_results['Component'] + \" \" + to_see_results['Description']\n",
    "\n",
    "actual = [to_see_results[\"Priority\"]]\n",
    "actual_df = pd.DataFrame(actual)\n",
    "actual_df = actual_df.transpose()\n",
    "actual_df\n",
    "\n",
    "Combined_Text = [to_see_results[\"Combined_Text\"]]\n",
    "df = pd.DataFrame(Combined_Text)\n",
    "df = df.transpose()\n",
    "df\n",
    "label_map = {'P1': 1, 'P2': 2, 'P3': 3, 'P4': 4, 'P5': 5}\n",
    "to_see_results['Priority'] = to_see_results['Priority'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe7952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "014b2c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level = list(range(1, 6))\n",
    "level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccce0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Cant disable a feature Update  (deprecated - u...      9\n",
      "1      build id wrong in the about dialog UI In win32...      9\n",
      "2      [JFace] ConfigureColumnsDialog does not work c...      9\n",
      "3      Widget is disposed in ControlExample SWT - run...      4\n",
      "4      An internal error occurred during: Initializin...      4\n",
      "...                                                  ...    ...\n",
      "17027                                                         0\n",
      "17028  [GTK/Linux] Blank Windows with GTK3 UI I start...      9\n",
      "17029                                                         0\n",
      "17030  Crash (MacOS) - getIvar SWT Process:         e...      3\n",
      "17031                                                         0\n",
      "\n",
      "[17032 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "new_df = priority_pipeline_without_assessment(data = to_see_results['Combined_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4073c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assessment = AccuracyAssessment(5)\n",
    "assessment.update_vals(to_see_results['Priority'], new_df[\"Predicted_Priority\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e38896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority P1: Precision=0.0035, Recall=0.0755, F-measure=0.0067\n",
      "Priority P2: Precision=0.0234, Recall=0.1013, F-measure=0.0380\n",
      "Priority P3: Precision=0.9752, Recall=0.7691, F-measure=0.8600\n",
      "Priority P4: Precision=0.0051, Recall=0.0755, F-measure=0.0095\n",
      "Priority P5: Precision=0.0029, Recall=0.0800, F-measure=0.0056\n"
     ]
    }
   ],
   "source": [
    "assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b52770af",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = pd.DataFrame(to_see_results['Priority'])\n",
    "predicted = pd.DataFrame(new_df[\"Predicted_Priority\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1656f4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-Analysis for Priority Levels: Precision = 0.2020, Recall=0.2203, F-measure=0.1839\n"
     ]
    }
   ],
   "source": [
    "assessment.macroAnalysis()\n",
    "#It treats each class equally, so classes with fewer instances have the same weight as those with more instances, which may not reflect the practical importance or impact of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39fa9124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-Analysis for Priority Levels: Precision = 0.4784, Recall=0.5234, F-measure=0.4999\n"
     ]
    }
   ],
   "source": [
    "assessment.microAnalysis()\n",
    "#different aggregation strategies: Micro-averaging gives equal weight to each instance\n",
    "# Macro-averaging gives equal weight to each class\n",
    "#It can be skewed by classes with more instances, giving them more influence over the final metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "172785bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use micro-averaging if you want to emphasize overall performance across all instances equally.\n",
    "#Use macro-averaging if you want to evaluate and compare performance across different classes equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fabdaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517613903240958"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment.accuracyOverall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95d1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
