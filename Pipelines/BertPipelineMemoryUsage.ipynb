{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba43ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b0e0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 21:46:12.519550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 21:46:12.568475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 21:46:12.568542: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-19 21:46:12.596185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 21:46:14.222658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home//nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation + '``'+ '`'+ ''+ ',' + '/')\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2d2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of GPUs: 8\n",
      "CUDA device name: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "torch.cuda.set_device(torch.device(\"cuda:0\"))\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b2ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Pipelines\n",
    "class LDATopicModelPipeline:\n",
    "    def __init__(self, lda_model_path, vectorizer_path):\n",
    "        self.lda_model_path = lda_model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.lda = None\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.lda = joblib.load(self.lda_model_path)\n",
    "        self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        \n",
    "    def topic_distributions(self, new_documents):\n",
    "        if self.lda is None or self.vectorizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if isinstance(new_documents[\"Combined_Text\"], list):\n",
    "            # If new_documents is a list of strings\n",
    "            texts = new_documents\n",
    "        elif isinstance(new_documents[\"Combined_Text\"], pd.Series):\n",
    "            # If new_documents is a Pandas Series (assuming it's a single column from a DataFrame)\n",
    "            texts = new_documents[\"Combined_Text\"].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a list, Pandas Series, or DataFrame of strings.\")\n",
    "            \n",
    "        x = self.vectorizer.transform(texts)\n",
    "        topic_distributions = self.lda.transform(x)\n",
    "        return topic_distributions\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def append_topics(self, data, topic_distributions):\n",
    "        topics = []\n",
    "        for topic_dist in topic_distributions:\n",
    "            dominant_topic = topic_dist.argmax()\n",
    "            topics.append(dominant_topic)\n",
    "            \n",
    "        data['Topic'] = topics\n",
    "        return data\n",
    "            \n",
    "#gives necessary files to process information\n",
    "#use through this syntax: topic_distributions = topic_model.topic_distributions(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc75e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training BERT model for text classification\n",
    "class BertTextClassification:\n",
    "    def __init__(self):\n",
    "        self.documents = None\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def tokenize_function(self, text):\n",
    "        encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        #print(\"Tokenized input IDs:\", encoding['input_ids'])\n",
    "        #print(\"Tokenized attention mask:\", encoding['attention_mask'])\n",
    "        return encoding\n",
    "\n",
    "    def load_model(self, topic_num):\n",
    "        #Use Trained Text Classifier Based on Topic Number\n",
    "         for x in [topic_num]:\n",
    "            self.model = joblib.load(f'/home//Files/BERTTrained/topic_pred_{x}.pkl')\n",
    "            self.model.to('cpu')\n",
    "            \n",
    "    def priority(self, documents):\n",
    "        self.documents = documents.copy()  # Ensure documents is a DataFrame\n",
    "        self.num_topics = documents['Topic'].drop_duplicates().values\n",
    "\n",
    "        for topic_num in self.num_topics:\n",
    "            self.load_model(topic_num)\n",
    "            print(f\"Processing Topic Number: {topic_num}\")  # Debug print\n",
    "            for i, text in enumerate(documents['Combined_Text']):  # Ensure you're iterating through the correct column\n",
    "                if self.documents.loc[i, 'Topic'] == topic_num:\n",
    "                    #print(f\"Processing Row: {i}\")  # Debug print\n",
    "                    encoding = self.tokenize_function(text)\n",
    "                    inputs = {\n",
    "                        'input_ids': encoding['input_ids'],\n",
    "                        'attention_mask': encoding['attention_mask']\n",
    "                    }\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        logits = outputs.logits\n",
    "                        if torch.any(torch.isnan(logits)):\n",
    "                            print(\"NaN detected in logits\")\n",
    "                        predicted_priority = logits.argmax(dim=1).cpu().numpy()[0]\n",
    "                        self.documents.at[i, \"Predicted_Priority\"] = predicted_priority\n",
    "        return self.documents\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9348bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Assessment\n",
    "class AccuracyAssessment:\n",
    "    def __init__(self, priority_levels):\n",
    "        self.actual_priority = None\n",
    "        self.predicted_priority = None\n",
    "        self.true_pos = 0\n",
    "        self.false_pos = 0\n",
    "        self.false_neg = 0\n",
    "        self.num_priority_levels = priority_levels\n",
    "        self.confusion_matrix = np.zeros((self.num_priority_levels, self.num_priority_levels), dtype=int)\n",
    "    \n",
    "    def update_vals(self, actual, predicted):\n",
    "        self.actual_priority = actual.astype(int)\n",
    "        self.predicted_priority = predicted.astype(int)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        for i in range(len(self.actual_priority)):\n",
    "            true_idx = self.actual_priority[i] - 1\n",
    "            pred_idx = self.predicted_priority[i] - 1\n",
    "            self.confusion_matrix[true_idx, pred_idx] += 1\n",
    "    \n",
    "    def calc_metrics(self, class_index):\n",
    "        self.true_pos = self.confusion_matrix[class_index, class_index]\n",
    "        self.false_pos = np.sum(self.confusion_matrix[:, class_index]) - self.true_pos\n",
    "        self.false_neg = np.sum(self.confusion_matrix[class_index, :]) - self.true_pos\n",
    "    \n",
    "    def precision(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_pos == 0:\n",
    "            return 0\n",
    "        precision = self.true_pos / (self.true_pos + self.false_pos)\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    def recall(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_neg == 0:\n",
    "            return 0\n",
    "        \n",
    "        recall = self.true_pos / (self.true_pos + self.false_neg)\n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    def fmeasure(self, class_index):\n",
    "        precision = self.precision(class_index)\n",
    "        recall = self.recall(class_index)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        \n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        return fmeasure\n",
    "    \n",
    "    def accuracyOverall(self):\n",
    "        accuratePriority = 0\n",
    "        for i in range(self.num_priority_levels):\n",
    "            self.calc_metrics(i)\n",
    "            accuratePriority += self.true_pos\n",
    "        accuratePriorities = accuratePriority / len(self.actual_priority)\n",
    "        return accuratePriorities\n",
    "    \n",
    "    def microAnalysis(self):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f_measures = []\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "            \n",
    "            precisions.append(precision_score)\n",
    "            recalls.append(recall_score)\n",
    "            f_measures.append(f_measure_score)\n",
    "        \n",
    "        micro_precision = sum(precisions) / sum(precisions + recalls)\n",
    "        micro_recall = sum(precisions) / sum(precisions + f_measures)\n",
    "        micro_fmeasure = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "        print(f\"Micro-Analysis for Priority Levels: Precision = {micro_precision:.4f}, Recall={micro_recall:.4f}, F-measure={micro_fmeasure:.4f}\")\n",
    "   \n",
    "    def macroAnalysis(self):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f_measures = []\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "            \n",
    "            precisions.append(precision_score)\n",
    "            recalls.append(recall_score)\n",
    "            f_measures.append(f_measure_score)\n",
    "\n",
    "        macro_precision = sum(precisions) / len(precisions)\n",
    "        macro_recall = sum(recalls) / len(recalls)\n",
    "        macro_fmeasure = sum(f_measures) / len(f_measures)\n",
    "        print(f\"Macro-Analysis for Priority Levels: Precision = {macro_precision:.4f}, Recall={macro_recall:.4f}, F-measure={macro_fmeasure:.4f}\")\n",
    "    def printAssessment(self):\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "    \n",
    "            print(f\"Priority P{i+1}: Precision={precision_score:.4f}, Recall={recall_score:.4f}, F-measure={f_measure_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b514ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes together in pipeline:\n",
    "def priority_pipeline_without_assessment(data):\n",
    "    #PreProcessing\n",
    "    #preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "    #data = preprocess.data_to_tokens(data)\n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    df['Combined_Text'] = df['Combined_Text'].fillna(' ')\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    textClass = BertTextClassification()\n",
    "    data = textClass.priority(df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472affd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "to_see_results = pd.read_csv('/home/extracted/test_dataset_notpreprocessed.csv')\n",
    "to_see_results.head()\n",
    "\n",
    "to_see_results['Combined_Text'] = to_see_results['Title'] + \" \" + to_see_results['Component'] + \" \" + to_see_results['Description']\n",
    "\n",
    "actual = [to_see_results[\"Priority\"]]\n",
    "actual_df = pd.DataFrame(actual)\n",
    "actual_df = actual_df.transpose()\n",
    "actual_df\n",
    "\n",
    "Combined_Text = [to_see_results[\"Combined_Text\"]]\n",
    "df = pd.DataFrame(Combined_Text)\n",
    "df = df.transpose()\n",
    "df\n",
    "label_map = {'P1': 1, 'P2': 2, 'P3': 3, 'P4': 4, 'P5': 5}\n",
    "to_see_results['Priority'] = to_see_results['Priority'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a063ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LatentDirichletAllocation from version 1.5.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.5.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Cant disable a feature Update  (deprecated - u...      9\n",
      "1      build id wrong in the about dialog UI In win32...      9\n",
      "2      [JFace] ConfigureColumnsDialog does not work c...      9\n",
      "3      Widget is disposed in ControlExample SWT - run...      4\n",
      "4      An internal error occurred during: Initializin...      4\n",
      "...                                                  ...    ...\n",
      "17027                                                         0\n",
      "17028  [GTK/Linux] Blank Windows with GTK3 UI I start...      9\n",
      "17029                                                         0\n",
      "17030  Crash (MacOS) - getIvar SWT Process:         e...      3\n",
      "17031                                                         0\n",
      "\n",
      "[17032 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Topic Number: 9\n",
      "Processing Topic Number: 4\n",
      "Processing Topic Number: 0\n",
      "Processing Topic Number: 8\n",
      "Processing Topic Number: 7\n",
      "Processing Topic Number: 5\n",
      "Processing Topic Number: 6\n",
      "Processing Topic Number: 3\n",
      "Processing Topic Number: 2\n",
      "Processing Topic Number: 1\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from memory_profiler import profile\n",
    "\n",
    "cProfile.run(\"new_df = priority_pipeline_without_assessment(data = to_see_results['Combined_Text'])\", 'output.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e277c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 19 23:03:12 2024    output.prof\n",
      "\n",
      "         75232243 function calls (67074301 primitive calls) in 4608.664 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 5615 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   1602/1    0.170    0.000 4609.003 4609.003 {built-in method builtins.exec}\n",
      "        1    0.041    0.041 4609.003 4609.003 <string>:1(<module>)\n",
      "        1    0.008    0.008 4608.961 4608.961 /tmp/ipykernel_244589/4204955630.py:2(priority_pipeline_without_assessment)\n",
      "        1    2.973    2.973 4589.813 4589.813 /tmp/ipykernel_244589/2725625287.py:21(priority)\n",
      "3917360/17032   13.144    0.000 4518.385    0.265 /home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/torch/nn/modules/module.py:1528(_wrapped_call_impl)\n",
      "3917360/17032  882.823    0.000 4518.297    0.265 /home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/torch/nn/modules/module.py:1534(_call_impl)\n",
      "    17032    0.560    0.000 4517.979    0.265 /home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1535(forward)\n",
      "    17032    1.424    0.000 4513.644    0.265 /home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:904(forward)\n",
      "    17032    3.091    0.000 4486.262    0.263 /home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:563(forward)\n",
      "   204384    4.481    0.000 4477.940    0.022 /home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:485(forward)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f560c192d10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats \n",
    "\n",
    "p = pstats.Stats('output.prof')\n",
    "\n",
    "#Sort the statistics by the cumulative time spent\n",
    "p.sort_stats('cumulative')\n",
    "\n",
    "#Print the statistics\n",
    "p.print_stats(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bea1b2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LatentDirichletAllocation from version 1.5.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.5.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Cant disable a feature Update  (deprecated - u...      9\n",
      "1      build id wrong in the about dialog UI In win32...      9\n",
      "2      [JFace] ConfigureColumnsDialog does not work c...      9\n",
      "3      Widget is disposed in ControlExample SWT - run...      4\n",
      "4      An internal error occurred during: Initializin...      4\n",
      "...                                                  ...    ...\n",
      "17027                                                         0\n",
      "17028  [GTK/Linux] Blank Windows with GTK3 UI I start...      9\n",
      "17029                                                         0\n",
      "17030  Crash (MacOS) - getIvar SWT Process:         e...      3\n",
      "17031                                                         0\n",
      "\n",
      "[17032 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//anaconda3/envs/secondenvi/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Topic Number: 9\n",
      "Processing Topic Number: 4\n",
      "Processing Topic Number: 0\n",
      "Processing Topic Number: 8\n",
      "Processing Topic Number: 7\n",
      "Processing Topic Number: 5\n",
      "Processing Topic Number: 6\n",
      "Processing Topic Number: 3\n",
      "Processing Topic Number: 2\n",
      "Processing Topic Number: 1\n",
      "Current memory usage: 0.6181840896606445 MB; Peak: 482.3677396774292 MB\n"
     ]
    }
   ],
   "source": [
    "import tracemalloc\n",
    "tracemalloc.start()\n",
    "priority_pipeline_without_assessment(data = to_see_results['Combined_Text'])\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage: {current / 1024 / 1024} MB; Peak: {peak / 1024 / 1024} MB\")\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f0e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a33c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
