{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204b13ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home//anaconda3/envs/firstenvi/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home//anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home//anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home//anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home//anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (4.66.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home//nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation + '``'+ '`'+ ''+ ',' + '/')\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7c2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Preprocessing subpipeline\n",
    "class PreprocessingPineline:\n",
    "    def __init__(self, stop_words, punctuation):\n",
    "        self.punctuation = punctuation\n",
    "        self.stop_words = stop_words\n",
    "        self.data = None\n",
    "        self.tokens = None        \n",
    "        \n",
    "    def data_to_tokens(self, data):\n",
    "        self.data = data\n",
    "        self.tokens = self.data.astype(str).apply(word_tokenize)\n",
    "        self.tokens = [[word for word in tokens if word.lower() not in stop_words and word not in punctuation] for tokens in self.tokens]\n",
    "        return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d819cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Topic Modeling subPipeline\n",
    "class LDATopicModelPipeline:\n",
    "    def __init__(self, lda_model_path, vectorizer_path):\n",
    "        self.lda_model_path = lda_model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.lda = None\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.lda = joblib.load(self.lda_model_path)\n",
    "        self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        \n",
    "    def topic_distributions(self, new_documents):\n",
    "        if self.lda is None or self.vectorizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if isinstance(new_documents[\"Combined_Text\"], list):\n",
    "            # If new_documents is a list of strings\n",
    "            texts = new_documents\n",
    "        elif isinstance(new_documents[\"Combined_Text\"], pd.Series):\n",
    "            # If new_documents is a Pandas Series (assuming it's a single column from a DataFrame)\n",
    "            texts = new_documents[\"Combined_Text\"].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a list, Pandas Series, or DataFrame of strings.\")\n",
    "            \n",
    "        x = self.vectorizer.transform(texts)\n",
    "        topic_distributions = self.lda.transform(x)\n",
    "        return topic_distributions\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def append_topics(self, data, topic_distributions):\n",
    "        topics = []\n",
    "        for topic_dist in topic_distributions:\n",
    "            dominant_topic = topic_dist.argmax()\n",
    "            topics.append(dominant_topic)\n",
    "            \n",
    "        data['Topic'] = topics\n",
    "        return data\n",
    "            \n",
    "#gives necessary files to process information\n",
    "#use through this syntax: topic_distributions = topic_model.topic_distributions(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c7cddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an Text Classification subpipeline\n",
    "class TextClassificationNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.documents = None\n",
    "        self.vectorizer = None\n",
    "        self.bayes_trained = None\n",
    "        \n",
    "        \n",
    "    def load_model(self, topic_num):\n",
    "        #Use Trained Text Classifier Based on Topic Number\n",
    "         for x in [topic_num]:\n",
    "            self.bayes_trained = joblib.load(f'topic_pred_{x}.pkl')\n",
    "            self.vectorizer = joblib.load(f'vec_{x}.pkl')\n",
    "    def priority(self, documents):\n",
    "        self.num_topics = documents['Topic'].drop_duplicates().values\n",
    "        documents[\"Predicted_Priority\"] = \"\" \n",
    "        self.documents = documents\n",
    "        \n",
    "        for topic_num in self.num_topics:\n",
    "            self.load_model(topic_num)\n",
    "            for i in self.documents.index: \n",
    "                if self.documents.loc[i, 'Topic'] == topic_num:\n",
    "                    text = self.documents.loc[i, \"Combined_Text\"]\n",
    "                    vector = self.vectorizer.transform([text]).reshape(1, -1)\n",
    "                    prediction = self.bayes_trained.predict(vector)\n",
    "                    self.documents.at[i, \"Predicted_Priority\"] = prediction[0]\n",
    "        return self.documents\n",
    "        \n",
    "    \n",
    "        #topic_prediction_function.predict(topic_validation_data.toarray())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2df03af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the equations for accuracy information\n",
    "class AccuracyAssessment:\n",
    "    def __init__(self, priority_levels):\n",
    "        self.actual_priority = None\n",
    "        self.predicted_priority = None\n",
    "        self.true_pos = None\n",
    "        self.false_pos = None\n",
    "        self.false_neg = None\n",
    "        self.num_priority_levels = priority_levels\n",
    "        self.confusion_matrix = np.zeros((self.num_priority_levels, self.num_priority_levels), dtype=int)\n",
    "    \n",
    "    def update_vals(self, actual, predicted):\n",
    "        self.actual_priority = actual.astype(int)\n",
    "        self.predicted_priority = predicted.astype(int)\n",
    "        for i in range(len(self.actual_priority)):\n",
    "            self.confusion_matrix[self.actual_priority[i] - 1, self.predicted_priority[i] - 1] += 1\n",
    "    \n",
    "    def calc_metrics(self, class_index):\n",
    "        self.true_pos = (self.actual_priority == self.predicted_priority).sum()\n",
    "        self.false_pos = np.sum(self.confusion_matrix[:, class_index]) - self.true_pos\n",
    "        self.false_neg = np.sum(self.confusion_matrix[class_index, :]) - self.true_pos\n",
    "    \n",
    "    def precision(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_pos == 0:\n",
    "            return 0\n",
    "        precision = self.true_pos / (self.true_pos + self.false_pos)\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    def recall(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_neg == 0:\n",
    "            return 0\n",
    "        \n",
    "        recall = self.true_pos / (self.true_pos + self.false_neg)\n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    def fmeasure(self, class_index):\n",
    "        precision = self.precision(class_index)\n",
    "        recall = self.recall(class_index)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        \n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        return fmeasure\n",
    "    \n",
    "    def printAssessment(self):\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "    \n",
    "            print(f\"Priority P{i+1}: Precision={precision_score:.4f}, Recall={recall_score:.4f}, F-measure={f_measure_score:.4f}\")\n",
    "\n",
    "#make sure the rows are formatted appropriately for the following:\n",
    "#assessment.update_vals(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc04cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline(data, actual):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "   # preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "   # data = preprocess.data_to_tokens(data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    data = nb.priority(df)\n",
    "    print(data[\"Predicted_Priority\"])\n",
    "    print(data.groupby('Predicted_Priority').sum())\n",
    "    \n",
    "    #Accuracy and Evaluation\n",
    "    assessment = AccuracyAssessment(priority_levels = 5)\n",
    "    assessment.update_vals(actual, data[\"Predicted_Priority\"])\n",
    "    assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d387007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline_without_assessment(self, data):\n",
    "    self.data = data\n",
    "    #PreProcessing\n",
    "    preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "    self.data = preprocess.data_to_tokens(self.data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    topic_distributions = topic_model.topic_distributions(self.data)\n",
    "    self.data = topic_model.append_topics(self.data, topic_distributions)\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    self.data = nb.priority(self.data)\n",
    "    return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45f5fd2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "to_see_results = pd.read_csv('/home/extracted/train_dataset_0.csv')\n",
    "to_see_results.head()\n",
    "\n",
    "actual = [to_see_results[\"Priority\"]]\n",
    "actual_df = pd.DataFrame(actual)\n",
    "actual_df = actual_df.transpose()\n",
    "actual_df\n",
    "\n",
    "Combined_Text = [to_see_results[\"Combined_Text\"]]\n",
    "df = pd.DataFrame(Combined_Text)\n",
    "df = df.transpose()\n",
    "df\n",
    "label_map = {'P1': 1, 'P2': 2, 'P3': 3, 'P4': 4, 'P5': 5}\n",
    "to_see_results['Priority'] = to_see_results['Priority'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe7952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6d187ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Usability issue with external editors (1GE6IRL...      9\n",
      "1      CC Discussion: local versioning (1GAT3PL) Team...      9\n",
      "2      Manage/unmanage support and policies (1GALAEG)...      9\n",
      "3      API: ISharingManager::load mapping vcm project...      9\n",
      "4      API - VCM event notification (1G8G6RR) Team Th...      9\n",
      "...                                                  ...    ...\n",
      "40869  wrong size computation for Link widget SWT I20...      6\n",
      "40870  Visibility Property for TableColumns SWT Prior...      9\n",
      "40871  Invalid required space shown on feature instal...      9\n",
      "40872  Widget disposed exception after updating incom...      4\n",
      "40873  Widget is disposed in WorkbenchContextSupport....      9\n",
      "\n",
      "[40874 rows x 2 columns]\n",
      "0        3\n",
      "1        3\n",
      "2        3\n",
      "3        3\n",
      "4        3\n",
      "        ..\n",
      "40869    3\n",
      "40870    3\n",
      "40871    3\n",
      "40872    3\n",
      "40873    3\n",
      "Name: Predicted_Priority, Length: 40874, dtype: object\n",
      "                                                        Combined_Text   Topic\n",
      "Predicted_Priority                                                           \n",
      "3                   Usability issue with external editors (1GE6IRL...  329764\n",
      "Priority P1: Precision=0.0000, Recall=18.2164, F-measure=0.0000\n",
      "Priority P2: Precision=0.0000, Recall=8.4137, F-measure=0.0000\n",
      "Priority P3: Precision=0.7991, Recall=1.0000, F-measure=0.8883\n",
      "Priority P4: Precision=0.0000, Recall=19.8916, F-measure=0.0000\n",
      "Priority P5: Precision=0.0000, Recall=36.4939, F-measure=0.0000\n"
     ]
    }
   ],
   "source": [
    "priority_pipeline(data = to_see_results['Combined_Text'], actual = to_see_results['Priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b2c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
