{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204b13ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (4.66.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rpierson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation + '``'+ '`'+ ''+ ',' + '/')\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7c2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Preprocessing subpipeline\n",
    "class PreprocessingPineline:\n",
    "    def __init__(self, stop_words, punctuation):\n",
    "        self.punctuation = punctuation\n",
    "        self.stop_words = stop_words\n",
    "        self.data = None\n",
    "        self.tokens = None        \n",
    "        \n",
    "    def data_to_tokens(self, data):\n",
    "        self.data = data\n",
    "        self.tokens = self.data.astype(str).apply(word_tokenize)\n",
    "        self.tokens = [[word for word in tokens if word.lower() not in stop_words and word not in punctuation] for tokens in self.tokens]\n",
    "        return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d819cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Topic Modeling subPipeline\n",
    "class LDATopicModelPipeline:\n",
    "    def __init__(self, lda_model_path, vectorizer_path):\n",
    "        self.lda_model_path = lda_model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.lda = None\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.lda = joblib.load(self.lda_model_path)\n",
    "        self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        \n",
    "    def topic_distributions(self, new_documents):\n",
    "        if self.lda is None or self.vectorizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if isinstance(new_documents[\"Combined_Text\"], list):\n",
    "            # If new_documents is a list of strings\n",
    "            texts = new_documents\n",
    "        elif isinstance(new_documents[\"Combined_Text\"], pd.Series):\n",
    "            # If new_documents is a Pandas Series (assuming it's a single column from a DataFrame)\n",
    "            texts = new_documents[\"Combined_Text\"].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a list, Pandas Series, or DataFrame of strings.\")\n",
    "            \n",
    "        x = self.vectorizer.transform(texts)\n",
    "        topic_distributions = self.lda.transform(x)\n",
    "        return topic_distributions\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def append_topics(self, data, topic_distributions):\n",
    "        topics = []\n",
    "        for topic_dist in topic_distributions:\n",
    "            dominant_topic = topic_dist.argmax()\n",
    "            topics.append(dominant_topic)\n",
    "            \n",
    "        data['Topic'] = topics\n",
    "        return data\n",
    "            \n",
    "#gives necessary files to process information\n",
    "#use through this syntax: topic_distributions = topic_model.topic_distributions(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7cddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an Text Classification subpipeline\n",
    "class TextClassificationNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.documents = None\n",
    "        self.vectorizer = None\n",
    "        self.bayes_trained = None\n",
    "        \n",
    "        \n",
    "    def load_model(self, topic_num):\n",
    "        #Use Trained Text Classifier Based on Topic Number\n",
    "         for x in [topic_num]:\n",
    "            self.bayes_trained = joblib.load(f'/home/rpierson/githubPierson/bayes/topic_pred_{x}.pkl')\n",
    "            self.vectorizer = joblib.load(f'/home/rpierson/githubPierson/bayes/vec_{x}.pkl')\n",
    "    def priority(self, documents):\n",
    "        self.num_topics = documents['Topic'].drop_duplicates().values\n",
    "        documents[\"Predicted_Priority\"] = \"\" \n",
    "        self.documents = documents\n",
    "        \n",
    "        for topic_num in self.num_topics:\n",
    "            self.load_model(topic_num)\n",
    "            for i in self.documents.index: \n",
    "                if self.documents.loc[i, 'Topic'] == topic_num:\n",
    "                    text = self.documents.loc[i, \"Combined_Text\"]\n",
    "                    vector = self.vectorizer.transform([text]).toarray().reshape(1, -1)\n",
    "                    prediction = self.bayes_trained.predict(vector)\n",
    "                    self.documents.at[i, \"Predicted_Priority\"] = prediction[0]\n",
    "        return self.documents\n",
    "        \n",
    "    \n",
    "        #topic_prediction_function.predict(topic_validation_data.toarray())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2df03af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the equations for accuracy information\n",
    "class AccuracyAssessment:\n",
    "    def __init__(self, priority_levels): \n",
    "        self.priority_levels = list(range(1, priority_levels + 1))\n",
    "        self.actual = None\n",
    "        self.predicted = None\n",
    "        self.true_level = []\n",
    "        self.pred_level = []\n",
    "        \n",
    "    def update_vals(self, actual, predicted):\n",
    "        self.actual = actual\n",
    "        self.predicted = predicted\n",
    "        \n",
    "    def tru_pred_levs(self, i):\n",
    "        \n",
    "        priority_level = self.priority_levels[i]\n",
    "        self.true_level = [self.actual[idx] for idx in range(len(self.actual)) if self.actual[idx] == priority_level]\n",
    "        self.pred_level = [self.predicted[idx] for idx in range(len(self.predicted)) if self.actual[idx] == priority_level]\n",
    "    \n",
    "    def accuracy(self, i):\n",
    "        self.tru_pred_levs(i)\n",
    "        accuracy = accuracy_score(self.true_level, self.pred_level)\n",
    "        return accuracy\n",
    "        \n",
    "    def precision(self, i):\n",
    "        self.tru_pred_levs(i)\n",
    "        precision = precision_score(self.true_level, self.pred_level, zero_division = 0, average = None)\n",
    "        return precision\n",
    "    \n",
    "    def recall(self, i):\n",
    "        self.tru_pred_levs(i)\n",
    "        recall = recall_score(self.true_level, self.pred_level, zero_division = 0, average = None)\n",
    "        return recall\n",
    "    \n",
    "    def fmeasure(self, i):\n",
    "        self.tru_pred_levs(i)\n",
    "        fmeasure = f1_score(self.true_level, self.pred_level, zero_division = 0, average = None)\n",
    "        return fmeasure\n",
    "    \n",
    "    def macroeval(self):\n",
    "        macro_precision = 0.0\n",
    "        macro_recall = 0.0\n",
    "        macro_fmeasure = 0.0\n",
    "        macro_accuracy = 0.0\n",
    "        \n",
    "        for i in range(self.num_priority_levels):\n",
    "            self.tru_pred_levs(i)\n",
    "            accuracy_score = accuracy_score(self.true_level, self.pred_level)\n",
    "            precision_score = precision_score(self.true_level, self.pred_level, zero_division = 0)\n",
    "            recall_score = recall_score(self.true_level, self.pred_level, zero_division = 0)\n",
    "            f_measure_score = f1_score(self.true_level, self.pred_level, zero_division = 0)\n",
    "            \n",
    "            macro_accuracy += accuracy_score\n",
    "            macro_precision += precision_score\n",
    "            macro_recall += recall_score\n",
    "            macro_fmeasure += f_measure_score\n",
    "        \n",
    "        num_levels = len(self.priority_levels)\n",
    "        macro_accuracy /= num_levels\n",
    "        macro_precision /= num_levels\n",
    "        macro_recall /= num_levels\n",
    "        macro_fmeasure /= num_levels\n",
    "        \n",
    "        return macro_accuracy, macro_precision, macro_recall, macro_fmeasure\n",
    "    \n",
    "    def printAssessment(self):\n",
    "        for i in range(len(self.priority_levels)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            accuracy_score = self.accuracy(i)\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)    \n",
    "            \n",
    "            for class_idx in range(len(precision_score)):  # Assuming all scores have the same length\n",
    "                precision = precision_score[class_idx]\n",
    "                recall = recall_score[class_idx]\n",
    "                f_measure = f_measure_score[class_idx]\n",
    "                print(f\"  Priority {class_idx}: Accuracy={accuracy_score:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F-measure={f_measure:.4f}\")\n",
    "            \n",
    "        macro_accuracy, macro_precision, macro_recall, macro_fmeasure = self.macroeval()\n",
    "        print(f\"Priority Overall: Accuracy={macro_accuracy:.4f}, Precision={macro_precision:.4f}, Recall={macro_recall:.4f}, F-measure={macro_fmeasure:.4f}\")\n",
    "\n",
    "#make sure the rows are formatted appropriately for the following:\n",
    "#assessment.update_vals(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc04cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline(data, actual):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "   # preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "   # data = preprocess.data_to_tokens(data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    data = nb.priority(df)\n",
    "    print(data[\"Predicted_Priority\"])\n",
    "    print(data.groupby('Predicted_Priority').sum())\n",
    "    \n",
    "    #Accuracy and Evaluation\n",
    "    assessment = AccuracyAssessment(priority_levels = 5)\n",
    "    assessment.update_vals(actual, data[\"Predicted_Priority\"])\n",
    "    assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d387007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline_without_assessment(data):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "   # preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "   # data = preprocess.data_to_tokens(data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    data = nb.priority(df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f5fd2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "to_see_results = pd.read_csv('/home/rpierson/PiersonREU/extracted/train_dataset_0.csv')\n",
    "to_see_results.head()\n",
    "\n",
    "actual = [to_see_results[\"Priority\"]]\n",
    "actual_df = pd.DataFrame(actual)\n",
    "actual_df = actual_df.transpose()\n",
    "actual_df\n",
    "\n",
    "Combined_Text = [to_see_results[\"Combined_Text\"]]\n",
    "df = pd.DataFrame(Combined_Text)\n",
    "df = df.transpose()\n",
    "df\n",
    "label_map = {'P1': 1, 'P2': 2, 'P3': 3, 'P4': 4, 'P5': 5}\n",
    "to_see_results['Priority'] = to_see_results['Priority'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe7952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d187ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Usability issue with external editors (1GE6IRL...      9\n",
      "1      CC Discussion: local versioning (1GAT3PL) Team...      9\n",
      "2      Manage/unmanage support and policies (1GALAEG)...      9\n",
      "3      API: ISharingManager::load mapping vcm project...      9\n",
      "4      API - VCM event notification (1G8G6RR) Team Th...      9\n",
      "...                                                  ...    ...\n",
      "40869  wrong size computation for Link widget SWT I20...      6\n",
      "40870  Visibility Property for TableColumns SWT Prior...      9\n",
      "40871  Invalid required space shown on feature instal...      9\n",
      "40872  Widget disposed exception after updating incom...      4\n",
      "40873  Widget is disposed in WorkbenchContextSupport....      9\n",
      "\n",
      "[40874 rows x 2 columns]\n",
      "0        3\n",
      "1        3\n",
      "2        3\n",
      "3        3\n",
      "4        3\n",
      "        ..\n",
      "40869    3\n",
      "40870    3\n",
      "40871    3\n",
      "40872    3\n",
      "40873    3\n",
      "Name: Predicted_Priority, Length: 40874, dtype: object\n",
      "                                                        Combined_Text   Topic\n",
      "Predicted_Priority                                                           \n",
      "1                   Cannot rename a project and then release it (1...   32717\n",
      "2                   Workspace files Team Thought it would be usefu...   41264\n",
      "3                   Usability issue with external editors (1GE6IRL...  209150\n",
      "4                   [CVS Repo View] go into and go back are very s...   23146\n",
      "5                   API - IResource.setLocal has problems (1G5TC8L...   23487\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m priority_pipeline(data \u001b[38;5;241m=\u001b[39m to_see_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined_Text\u001b[39m\u001b[38;5;124m'\u001b[39m], actual \u001b[38;5;241m=\u001b[39m to_see_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mpriority_pipeline\u001b[0;34m(data, actual)\u001b[0m\n\u001b[1;32m     26\u001b[0m assessment \u001b[38;5;241m=\u001b[39m AccuracyAssessment(priority_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     27\u001b[0m assessment\u001b[38;5;241m.\u001b[39mupdate_vals(actual, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted_Priority\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m assessment\u001b[38;5;241m.\u001b[39mprintAssessment()\n",
      "Cell \u001b[0;32mIn[32], line 76\u001b[0m, in \u001b[0;36mAccuracyAssessment.printAssessment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m f_measure_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmeasure(i)    \n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(precision_score)):  \u001b[38;5;66;03m# Assuming all scores have the same length\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     precision \u001b[38;5;241m=\u001b[39m precision_score_val[class_idx]\n\u001b[1;32m     77\u001b[0m     recall \u001b[38;5;241m=\u001b[39m recall_score_val[class_idx]\n\u001b[1;32m     78\u001b[0m     f_measure \u001b[38;5;241m=\u001b[39m f_measure_score_val[class_idx]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_score_val' is not defined"
     ]
    }
   ],
   "source": [
    "priority_pipeline(data = to_see_results['Combined_Text'], actual = to_see_results['Priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "014b2c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level = list(range(1, 6))\n",
    "level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccce0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Usability issue with external editors (1GE6IRL...      9\n",
      "1      CC Discussion: local versioning (1GAT3PL) Team...      9\n",
      "2      Manage/unmanage support and policies (1GALAEG)...      9\n",
      "3      API: ISharingManager::load mapping vcm project...      9\n",
      "4      API - VCM event notification (1G8G6RR) Team Th...      9\n",
      "...                                                  ...    ...\n",
      "40869  wrong size computation for Link widget SWT I20...      6\n",
      "40870  Visibility Property for TableColumns SWT Prior...      9\n",
      "40871  Invalid required space shown on feature instal...      9\n",
      "40872  Widget disposed exception after updating incom...      4\n",
      "40873  Widget is disposed in WorkbenchContextSupport....      9\n",
      "\n",
      "[40874 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "new_df = priority_pipeline_without_assessment(data = to_see_results['Combined_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4073c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Priority 0: Accuracy=0.8588, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 1: Accuracy=0.8588, Precision=1.0000, Recall=0.8588, F-measure=0.9241\n",
      "  Priority 2: Accuracy=0.8588, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 3: Accuracy=0.8588, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 0: Accuracy=0.8132, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 1: Accuracy=0.8132, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 2: Accuracy=0.8132, Precision=1.0000, Recall=0.8132, F-measure=0.8970\n",
      "  Priority 3: Accuracy=0.8132, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 4: Accuracy=0.8132, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 0: Accuracy=0.8977, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 1: Accuracy=0.8977, Precision=1.0000, Recall=0.8977, F-measure=0.9461\n",
      "  Priority 2: Accuracy=0.8977, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 0: Accuracy=0.9989, Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "  Priority 1: Accuracy=0.9989, Precision=1.0000, Recall=0.9989, F-measure=0.9994\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AccuracyAssessment' object has no attribute 'num_priority_levels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m assessment \u001b[38;5;241m=\u001b[39m AccuracyAssessment(priority_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m assessment\u001b[38;5;241m.\u001b[39mupdate_vals(to_see_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m], new_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted_Priority\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m assessment\u001b[38;5;241m.\u001b[39mprintAssessment()\n",
      "Cell \u001b[0;32mIn[39], line 81\u001b[0m, in \u001b[0;36mAccuracyAssessment.printAssessment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         f_measure \u001b[38;5;241m=\u001b[39m f_measure_score[class_idx]\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Priority \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F-measure=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_measure\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m macro_accuracy, macro_precision, macro_recall, macro_fmeasure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmacroeval()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPriority Overall: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F-measure=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_fmeasure\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 46\u001b[0m, in \u001b[0;36mAccuracyAssessment.macroeval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m macro_fmeasure \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     44\u001b[0m macro_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_priority_levels):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtru_pred_levs(i)\n\u001b[1;32m     48\u001b[0m     accuracy_score \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_level, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_level)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AccuracyAssessment' object has no attribute 'num_priority_levels'"
     ]
    }
   ],
   "source": [
    "assessment = AccuracyAssessment(priority_levels = 5)\n",
    "assessment.update_vals(to_see_results['Priority'], new_df[\"Predicted_Priority\"])\n",
    "assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52770af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
