{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204b13ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (4.66.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rpierson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation + '``'+ '`'+ ''+ ',' + '/')\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7c2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Preprocessing subpipeline\n",
    "class PreprocessingPineline:\n",
    "    def __init__(self, stop_words, punctuation):\n",
    "        self.punctuation = punctuation\n",
    "        self.stop_words = stop_words\n",
    "        self.data = None\n",
    "        self.tokens = None        \n",
    "        \n",
    "    def data_to_tokens(self, data):\n",
    "        self.data = data\n",
    "        self.tokens = self.data.astype(str).apply(word_tokenize)\n",
    "        self.tokens = [[word for word in tokens if word.lower() not in stop_words and word not in punctuation] for tokens in self.tokens]\n",
    "        return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d819cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Topic Modeling subPipeline\n",
    "class LDATopicModelPipeline:\n",
    "    def __init__(self, lda_model_path, vectorizer_path):\n",
    "        self.lda_model_path = lda_model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.lda = None\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.lda = joblib.load(self.lda_model_path)\n",
    "        self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        \n",
    "    def topic_distributions(self, new_documents):\n",
    "        if self.lda is None or self.vectorizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if isinstance(new_documents[\"Combined_Text\"], list):\n",
    "            # If new_documents is a list of strings\n",
    "            texts = new_documents\n",
    "        elif isinstance(new_documents[\"Combined_Text\"], pd.Series):\n",
    "            # If new_documents is a Pandas Series (assuming it's a single column from a DataFrame)\n",
    "            texts = new_documents[\"Combined_Text\"].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a list, Pandas Series, or DataFrame of strings.\")\n",
    "            \n",
    "        x = self.vectorizer.transform(texts)\n",
    "        topic_distributions = self.lda.transform(x)\n",
    "        return topic_distributions\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def append_topics(self, data, topic_distributions):\n",
    "        topics = []\n",
    "        for topic_dist in topic_distributions:\n",
    "            dominant_topic = topic_dist.argmax()\n",
    "            topics.append(dominant_topic)\n",
    "            \n",
    "        data['Topic'] = topics\n",
    "        return data\n",
    "            \n",
    "#gives necessary files to process information\n",
    "#use through this syntax: topic_distributions = topic_model.topic_distributions(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7cddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an Text Classification subpipeline\n",
    "class TextClassificationNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.documents = None\n",
    "        self.vectorizer = None\n",
    "        self.bayes_trained = None\n",
    "        \n",
    "        \n",
    "    def load_model(self, topic_num):\n",
    "        #Use Trained Text Classifier Based on Topic Number\n",
    "         for x in [topic_num]:\n",
    "            self.bayes_trained = joblib.load(f'/home/rpierson/githubPierson/bayes/topic_pred_{x}.pkl')\n",
    "            self.vectorizer = joblib.load(f'/home/rpierson/githubPierson/bayes/vec_{x}.pkl')\n",
    "    def priority(self, documents):\n",
    "        self.num_topics = documents['Topic'].drop_duplicates().values\n",
    "        documents[\"Predicted_Priority\"] = \"\" \n",
    "        self.documents = documents\n",
    "        \n",
    "        for topic_num in self.num_topics:\n",
    "            self.load_model(topic_num)\n",
    "            for i in self.documents.index: \n",
    "                if self.documents.loc[i, 'Topic'] == topic_num:\n",
    "                    text = self.documents.loc[i, \"Combined_Text\"]\n",
    "                    vector = self.vectorizer.transform([text]).toarray().reshape(1, -1)\n",
    "                    prediction = self.bayes_trained.predict(vector)\n",
    "                    self.documents.at[i, \"Predicted_Priority\"] = prediction[0]\n",
    "        return self.documents\n",
    "        \n",
    "    \n",
    "        #topic_prediction_function.predict(topic_validation_data.toarray())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2df03af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(actual, predicted):\n",
    "    self.true_level[priority_level] = []\n",
    "    self.pred_level[priority_level] = []\n",
    "        \n",
    "    for idx in range(len(self.actual)):\n",
    "        if self.actual[idx] == priority_level:\n",
    "            self.true_level[priority_level].append(self.actual[idx])\n",
    "            self.pred_level[priority_level].append(self.predicted[idx] if self.actual[idx] == priority_level else 6)\n",
    "            print(self.true_level[priority_level], self.pred_level[priority_level])        \n",
    "    accuracy = accuracy_score(actual, predicted)\n",
    "    return accuracy\n",
    "    \n",
    "#def precision(actual, predicted, priNum):\n",
    "    \n",
    "    \n",
    "#def recall(actual, predicted, priNum):\n",
    "    \n",
    "    \n",
    "#def fmeasure(actual, predicted, priNum):\n",
    "    \n",
    "    \n",
    "#def macroeval(actual, predicted):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc04cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline(data, actual):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "   # preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "   # data = preprocess.data_to_tokens(data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    data = nb.priority(df)\n",
    "    print(data[\"Predicted_Priority\"])\n",
    "    print(data.groupby('Predicted_Priority').sum())\n",
    "    \n",
    "    #Accuracy and Evaluation\n",
    "    assessment = AccuracyAssessment(priority_levels = 5)\n",
    "    assessment.update_vals(actual, data[\"Predicted_Priority\"])\n",
    "    assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d387007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline_without_assessment(data):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "   # preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "   # data = preprocess.data_to_tokens(data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    data = nb.priority(df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f5fd2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "to_see_results = pd.read_csv('/home/rpierson/PiersonREU/extracted/train_dataset_0.csv')\n",
    "to_see_results.head()\n",
    "\n",
    "actual = [to_see_results[\"Priority\"]]\n",
    "actual_df = pd.DataFrame(actual)\n",
    "actual_df = actual_df.transpose()\n",
    "actual_df\n",
    "\n",
    "Combined_Text = [to_see_results[\"Combined_Text\"]]\n",
    "df = pd.DataFrame(Combined_Text)\n",
    "df = df.transpose()\n",
    "df\n",
    "label_map = {'P1': 1, 'P2': 2, 'P3': 3, 'P4': 4, 'P5': 5}\n",
    "to_see_results['Priority'] = to_see_results['Priority'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe7952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d187ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Usability issue with external editors (1GE6IRL...      9\n",
      "1      CC Discussion: local versioning (1GAT3PL) Team...      9\n",
      "2      Manage/unmanage support and policies (1GALAEG)...      9\n",
      "3      API: ISharingManager::load mapping vcm project...      9\n",
      "4      API - VCM event notification (1G8G6RR) Team Th...      9\n",
      "...                                                  ...    ...\n",
      "40869  wrong size computation for Link widget SWT I20...      6\n",
      "40870  Visibility Property for TableColumns SWT Prior...      9\n",
      "40871  Invalid required space shown on feature instal...      9\n",
      "40872  Widget disposed exception after updating incom...      4\n",
      "40873  Widget is disposed in WorkbenchContextSupport....      9\n",
      "\n",
      "[40874 rows x 2 columns]\n",
      "0        3\n",
      "1        3\n",
      "2        3\n",
      "3        3\n",
      "4        3\n",
      "        ..\n",
      "40869    3\n",
      "40870    3\n",
      "40871    3\n",
      "40872    3\n",
      "40873    3\n",
      "Name: Predicted_Priority, Length: 40874, dtype: object\n",
      "                                                        Combined_Text   Topic\n",
      "Predicted_Priority                                                           \n",
      "1                   Cannot rename a project and then release it (1...   32717\n",
      "2                   Workspace files Team Thought it would be usefu...   41264\n",
      "3                   Usability issue with external editors (1GE6IRL...  209150\n",
      "4                   [CVS Repo View] go into and go back are very s...   23146\n",
      "5                   API - IResource.setLocal has problems (1G5TC8L...   23487\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m priority_pipeline(data \u001b[38;5;241m=\u001b[39m to_see_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined_Text\u001b[39m\u001b[38;5;124m'\u001b[39m], actual \u001b[38;5;241m=\u001b[39m to_see_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mpriority_pipeline\u001b[0;34m(data, actual)\u001b[0m\n\u001b[1;32m     26\u001b[0m assessment \u001b[38;5;241m=\u001b[39m AccuracyAssessment(priority_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     27\u001b[0m assessment\u001b[38;5;241m.\u001b[39mupdate_vals(actual, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted_Priority\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m assessment\u001b[38;5;241m.\u001b[39mprintAssessment()\n",
      "Cell \u001b[0;32mIn[32], line 76\u001b[0m, in \u001b[0;36mAccuracyAssessment.printAssessment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m f_measure_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmeasure(i)    \n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(precision_score)):  \u001b[38;5;66;03m# Assuming all scores have the same length\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     precision \u001b[38;5;241m=\u001b[39m precision_score_val[class_idx]\n\u001b[1;32m     77\u001b[0m     recall \u001b[38;5;241m=\u001b[39m recall_score_val[class_idx]\n\u001b[1;32m     78\u001b[0m     f_measure \u001b[38;5;241m=\u001b[39m f_measure_score_val[class_idx]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_score_val' is not defined"
     ]
    }
   ],
   "source": [
    "priority_pipeline(data = to_see_results['Combined_Text'], actual = to_see_results['Priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "014b2c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level = list(range(1, 6))\n",
    "level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccce0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Usability issue with external editors (1GE6IRL...      9\n",
      "1      CC Discussion: local versioning (1GAT3PL) Team...      9\n",
      "2      Manage/unmanage support and policies (1GALAEG)...      9\n",
      "3      API: ISharingManager::load mapping vcm project...      9\n",
      "4      API - VCM event notification (1G8G6RR) Team Th...      9\n",
      "...                                                  ...    ...\n",
      "40869  wrong size computation for Link widget SWT I20...      6\n",
      "40870  Visibility Property for TableColumns SWT Prior...      9\n",
      "40871  Invalid required space shown on feature instal...      9\n",
      "40872  Widget disposed exception after updating incom...      4\n",
      "40873  Widget is disposed in WorkbenchContextSupport....      9\n",
      "\n",
      "[40874 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "new_df = priority_pipeline_without_assessment(data = to_see_results['Combined_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b4073c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8588356517259145"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment = AccuracyAssessment(5)\n",
    "assessment.update_vals(to_see_results['Priority'], new_df[\"Predicted_Priority\"])\n",
    "assessment.accuracy(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b52770af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40869</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40870</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40871</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40872</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40873</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40874 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Priority\n",
       "0             3\n",
       "1             3\n",
       "2             3\n",
       "3             3\n",
       "4             3\n",
       "...         ...\n",
       "40869         3\n",
       "40870         3\n",
       "40871         3\n",
       "40872         3\n",
       "40873         3\n",
       "\n",
       "[40874 rows x 1 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = pd.DataFrame(to_see_results['Priority'])\n",
    "predicted = pd.DataFrame(new_df[\"Predicted_Priority\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "275f4d8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(actual)):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual\u001b[38;5;241m.\u001b[39mloc[idx,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m priority_level:\n\u001b[0;32m----> 7\u001b[0m         true_level[priority_level]\u001b[38;5;241m.\u001b[39mappend(actual[idx], index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m         pred_level[priority_level]\u001b[38;5;241m.\u001b[39mappend(predicted[idx] \u001b[38;5;28;01mif\u001b[39;00m actual[idx] \u001b[38;5;241m==\u001b[39m priority_level \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m6\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m         accuracy_result \u001b[38;5;241m=\u001b[39m accuracy_result\u001b[38;5;241m.\u001b[39mappend(accuracy_score(true_level[priority_level], pred_level[priority_level]))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "true_level = []\n",
    "pred_level = []\n",
    "priority_level = 1\n",
    "        \n",
    "for idx in range(len(actual)):\n",
    "    if actual.loc[idx,'Priority'] == priority_level:\n",
    "        true_level[priority_level].append(actual[idx], index = False)\n",
    "        pred_level[priority_level].append(predicted[idx] if actual[idx] == priority_level else 6, index = False)\n",
    "        accuracy_result = accuracy_result.append(accuracy_score(true_level[priority_level], pred_level[priority_level]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7a676ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988826815642458"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment.accuracy(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "065e7a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905186837702176"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment.accuracy(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656f4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
