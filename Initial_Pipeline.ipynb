{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204b13ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: tqdm in /home/rpierson/anaconda3/envs/firstenvi/lib/python3.11/site-packages (from nltk) (4.66.4)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rpierson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation + '``'+ '`'+ ''+ ',' + '/')\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7c2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Preprocessing subpipeline\n",
    "class PreprocessingPineline:\n",
    "    def __init__(self, stop_words, punctuation):\n",
    "        self.punctuation = punctuation\n",
    "        self.stop_words = stop_words\n",
    "        self.data = None\n",
    "        self.tokens = None        \n",
    "        \n",
    "    def data_to_tokens(self, data):\n",
    "        self.data = data\n",
    "        self.tokens = self.data.astype(str).apply(word_tokenize)\n",
    "        self.tokens = [[word for word in tokens if word.lower() not in stop_words and word not in punctuation] for tokens in self.tokens]\n",
    "        return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d819cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Topic Modeling subPipeline\n",
    "class LDATopicModelPipeline:\n",
    "    def __init__(self, lda_model_path, vectorizer_path):\n",
    "        self.lda_model_path = lda_model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.lda = None\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.lda = joblib.load(self.lda_model_path)\n",
    "        self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        \n",
    "    def topic_distributions(self, new_documents):\n",
    "        if self.lda is None or self.vectorizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if isinstance(new_documents[\"Combined_Text\"], list):\n",
    "            # If new_documents is a list of strings\n",
    "            texts = new_documents\n",
    "        elif isinstance(new_documents[\"Combined_Text\"], pd.Series):\n",
    "            # If new_documents is a Pandas Series (assuming it's a single column from a DataFrame)\n",
    "            texts = new_documents[\"Combined_Text\"].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a list, Pandas Series, or DataFrame of strings.\")\n",
    "            \n",
    "        x = self.vectorizer.transform(texts)\n",
    "        topic_distributions = self.lda.transform(x)\n",
    "        return topic_distributions\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def append_topics(self, data, topic_distributions):\n",
    "        topics = []\n",
    "        for topic_dist in topic_distributions:\n",
    "            dominant_topic = topic_dist.argmax()\n",
    "            topics.append(dominant_topic)\n",
    "            \n",
    "        data['Topic'] = topics\n",
    "        return data\n",
    "            \n",
    "#gives necessary files to process information\n",
    "#use through this syntax: topic_distributions = topic_model.topic_distributions(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c7cddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an Text Classification subpipeline\n",
    "class TextClassificationNaiveBayes:\n",
    "    def __init__(self, vectorizer):\n",
    "        self.documents = None\n",
    "        self.vectorizer = vectorizer\n",
    "        self.bayes_trained = None\n",
    "        \n",
    "        \n",
    "    def load_model(self, topic_num):\n",
    "        #Use Trained Text Classifier Based on Topic Number\n",
    "         for x in [topic_num]:\n",
    "            self.bayes_trained = joblib.load(f'/home/rpierson/githubPierson/bayes/topic_pred_{x}.pkl')\n",
    "        \n",
    "    def priority(self, documents):\n",
    "        self.num_topics = documents['Topic'].drop_duplicates().values\n",
    "        documents[\"Predicted_Priority\"] = \"\" \n",
    "        self.documents = documents\n",
    "        \n",
    "        for topic_num in self.num_topics:\n",
    "            self.load_model(topic_num)\n",
    "            for i in self.documents.index: \n",
    "                if self.documents.loc[i, 'Topic'] == topic_num:\n",
    "                    text = self.documents.loc[i, \"Combined_Text\"]\n",
    "                    vector = self.vectorizer.transform([text])\n",
    "                    vector = vector.toarray().reshape(1, -1)\n",
    "                    prediction = self.bayes_trained.predict(vector)\n",
    "                    self.documents.at[i, \"Predicted_Priority\"] = prediction[0]\n",
    "        \n",
    "    \n",
    "        #topic_prediction_function.predict(topic_validation_data.toarray())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df03af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the equations for accuracy information\n",
    "class AccuracyAssessment:\n",
    "    def __init__(self, priority_levels):\n",
    "        self.actual_priority = None\n",
    "        self.predicted_priority = None\n",
    "        self.true_pos = None\n",
    "        self.false_pos = None\n",
    "        self.false_neg = None\n",
    "        self.num_priority_levels = priority_levels\n",
    "        self.confusion_matrix = np.zeros((self.num_priority_levels, self.num_priority_levels), dtype=int)\n",
    "    \n",
    "    def update_vals(self, actual, predicted):\n",
    "        self.actual_priority = actual\n",
    "        self.predicted_priority = predicted\n",
    "        for i in range(len(self.actual_priority)):\n",
    "            self.confusion_matrix[self.actual_priority[i] - 1, self.predicted_priority[i] - 1] += 1\n",
    "    \n",
    "    def calc_metrics(self, class_index):\n",
    "        self.true_pos = (actual_priority == predicted_priority).sum()\n",
    "        self.false_pos = np.sum(self.confusion_matrix[:, class_index]) - self.true_pos\n",
    "        self.false_neg = np.sum(self.confusion_matrix[class_index, :]) - self.true_pos\n",
    "    \n",
    "    def precision(self, class_index):\n",
    "        AccuracyAssessment.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_pos == 0:\n",
    "            return 0\n",
    "        precision = self.true_pos / (self.true_pos + self.false_pos)\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    def recall(self, class_index):\n",
    "        AccuracyAssessment.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_neg == 0:\n",
    "            return 0\n",
    "        \n",
    "        recall = self.true_pos / (self.true_pos + self.false_neg)\n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    def fmeasure(self, class_index):\n",
    "        precision = AccuracyAssessment.precision(class_index)\n",
    "        recall = AccuracyAssessment.recall(class_index)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        \n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        return fmeasure\n",
    "    \n",
    "    def printAssessment(self):\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = AccuracyAssessment.precision(i)\n",
    "            recall_score = AccuracyAssessment.recall(i)\n",
    "            f_measure_score = AccuracyAssessment.fmeasure(i)\n",
    "    \n",
    "            print(f\"Priority P{i+1}: Precision={precision_score:.4f}, Recall={recall_score:.4f}, F-measure={f_measure_score:.4f}\")\n",
    "\n",
    "#make sure the rows are formatted appropriately for the following:\n",
    "#assessment.update_vals(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc04cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline(data, actual):\n",
    "    data = data\n",
    "    #PreProcessing\n",
    "   # preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "   # data = preprocess.data_to_tokens(data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes(vectorizer)\n",
    "    data = nb.priority(df)\n",
    "    \n",
    "    #Accuracy and Evaluation\n",
    "    assessment = AccuracyAssessment(priority_levels = 5)\n",
    "    assessment.update_values(actual, data[\"Predicted_Priority\"])\n",
    "    assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d387007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_pipeline_without_assessment(self, data):\n",
    "    self.data = data\n",
    "    #PreProcessing\n",
    "    preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "    self.data = preprocess.data_to_tokens(self.data)\n",
    "    \n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    topic_distributions = topic_model.topic_distributions(self.data)\n",
    "    self.data = topic_model.append_topics(self.data, topic_distributions)\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    nb = TextClassificationNaiveBayes()\n",
    "    self.data = nb.priority(self.data)\n",
    "    return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f5fd2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combined_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Usability issue with external editors (1GE6IRL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC Discussion: local versioning (1GAT3PL) Team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manage/unmanage support and policies (1GALAEG)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>API: ISharingManager::load mapping vcm project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>API - VCM event notification (1G8G6RR) Team Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40869</th>\n",
       "      <td>wrong size computation for Link widget SWT I20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40870</th>\n",
       "      <td>Visibility Property for TableColumns SWT Prior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40871</th>\n",
       "      <td>Invalid required space shown on feature instal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40872</th>\n",
       "      <td>Widget disposed exception after updating incom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40873</th>\n",
       "      <td>Widget is disposed in WorkbenchContextSupport....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40874 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Combined_Text\n",
       "0      Usability issue with external editors (1GE6IRL...\n",
       "1      CC Discussion: local versioning (1GAT3PL) Team...\n",
       "2      Manage/unmanage support and policies (1GALAEG)...\n",
       "3      API: ISharingManager::load mapping vcm project...\n",
       "4      API - VCM event notification (1G8G6RR) Team Th...\n",
       "...                                                  ...\n",
       "40869  wrong size computation for Link widget SWT I20...\n",
       "40870  Visibility Property for TableColumns SWT Prior...\n",
       "40871  Invalid required space shown on feature instal...\n",
       "40872  Widget disposed exception after updating incom...\n",
       "40873  Widget is disposed in WorkbenchContextSupport....\n",
       "\n",
       "[40874 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "to_see_results = pd.read_csv('/home/rpierson/PiersonREU/extracted/train_dataset_0.csv')\n",
    "to_see_results.head()\n",
    "\n",
    "actual = [to_see_results[\"Priority\"]]\n",
    "actual_df = pd.DataFrame(actual)\n",
    "actual_df = actual_df.transpose()\n",
    "actual_df\n",
    "\n",
    "Combined_Text = [to_see_results[\"Combined_Text\"]]\n",
    "df = pd.DataFrame(Combined_Text)\n",
    "df = df.transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe7952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6d187ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Usability issue with external editors (1GE6IRL...      9\n",
      "1      CC Discussion: local versioning (1GAT3PL) Team...      9\n",
      "2      Manage/unmanage support and policies (1GALAEG)...      9\n",
      "3      API: ISharingManager::load mapping vcm project...      9\n",
      "4      API - VCM event notification (1G8G6RR) Team Th...      9\n",
      "...                                                  ...    ...\n",
      "40869  wrong size computation for Link widget SWT I20...      6\n",
      "40870  Visibility Property for TableColumns SWT Prior...      9\n",
      "40871  Invalid required space shown on feature instal...      9\n",
      "40872  Widget disposed exception after updating incom...      4\n",
      "40873  Widget is disposed in WorkbenchContextSupport....      9\n",
      "\n",
      "[40874 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 219686 features, but GaussianNB is expecting 42405 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m priority_pipeline(data \u001b[38;5;241m=\u001b[39m to_see_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined_Text\u001b[39m\u001b[38;5;124m'\u001b[39m], actual \u001b[38;5;241m=\u001b[39m to_see_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mpriority_pipeline\u001b[0;34m(data, actual)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m nb \u001b[38;5;241m=\u001b[39m TextClassificationNaiveBayes(vectorizer)\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mpriority(df)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#Accuracy and Evaluation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m assessment \u001b[38;5;241m=\u001b[39m AccuracyAssessment(priority_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 26\u001b[0m, in \u001b[0;36mTextClassificationNaiveBayes.priority\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     24\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mtransform([text])\n\u001b[1;32m     25\u001b[0m vector \u001b[38;5;241m=\u001b[39m vector\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbayes_trained\u001b[38;5;241m.\u001b[39mpredict(vector)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted_Priority\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/firstenvi/lib/python3.11/site-packages/sklearn/naive_bayes.py:101\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[1;32m    102\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/envs/firstenvi/lib/python3.11/site-packages/sklearn/naive_bayes.py:269\u001b[0m, in \u001b[0;36mGaussianNB._check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/firstenvi/lib/python3.11/site-packages/sklearn/base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/firstenvi/lib/python3.11/site-packages/sklearn/base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 219686 features, but GaussianNB is expecting 42405 features as input."
     ]
    }
   ],
   "source": [
    "priority_pipeline(data = to_see_results['Combined_Text'], actual = to_see_results['Priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b2c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
