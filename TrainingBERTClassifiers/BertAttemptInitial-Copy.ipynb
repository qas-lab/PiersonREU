{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba43ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b0e0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 21:39:52.273721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 21:39:52.304247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 21:39:52.304281: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-19 21:39:52.323454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 21:39:53.174612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/rpierson/anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rpierson/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation + '``'+ '`'+ ''+ ',' + '/')\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2d2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of GPUs: 8\n",
      "CUDA device name: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "torch.cuda.set_device(torch.device(\"cuda:0\"))\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b2ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Pipelines\n",
    "class LDATopicModelPipeline:\n",
    "    def __init__(self, lda_model_path, vectorizer_path):\n",
    "        self.lda_model_path = lda_model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.lda = None\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.lda = joblib.load(self.lda_model_path)\n",
    "        self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        \n",
    "    def topic_distributions(self, new_documents):\n",
    "        if self.lda is None or self.vectorizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if isinstance(new_documents[\"Combined_Text\"], list):\n",
    "            # If new_documents is a list of strings\n",
    "            texts = new_documents\n",
    "        elif isinstance(new_documents[\"Combined_Text\"], pd.Series):\n",
    "            # If new_documents is a Pandas Series (assuming it's a single column from a DataFrame)\n",
    "            texts = new_documents[\"Combined_Text\"].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a list, Pandas Series, or DataFrame of strings.\")\n",
    "            \n",
    "        x = self.vectorizer.transform(texts)\n",
    "        topic_distributions = self.lda.transform(x)\n",
    "        return topic_distributions\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def append_topics(self, data, topic_distributions):\n",
    "        topics = []\n",
    "        for topic_dist in topic_distributions:\n",
    "            dominant_topic = topic_dist.argmax()\n",
    "            topics.append(dominant_topic)\n",
    "            \n",
    "        data['Topic'] = topics\n",
    "        return data\n",
    "            \n",
    "#gives necessary files to process information\n",
    "#use through this syntax: topic_distributions = topic_model.topic_distributions(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc75e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training BERT model for text classification\n",
    "class BertTextClassification:\n",
    "    def __init__(self):\n",
    "        self.documents = None\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def tokenize_function(self, text):\n",
    "        encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        #print(\"Tokenized input IDs:\", encoding['input_ids'])\n",
    "        #print(\"Tokenized attention mask:\", encoding['attention_mask'])\n",
    "        return encoding\n",
    "\n",
    "    def load_model(self, topic_num):\n",
    "        #Use Trained Text Classifier Based on Topic Number\n",
    "         for x in [topic_num]:\n",
    "            self.model = joblib.load(f'/home/rpierson/Files/BERTTrained/topic_pred_{x}.pkl')\n",
    "            self.model.to('cpu')\n",
    "            \n",
    "    def priority(self, documents):\n",
    "        self.documents = documents.copy()  # Ensure documents is a DataFrame\n",
    "        self.num_topics = documents['Topic'].drop_duplicates().values\n",
    "\n",
    "        for topic_num in self.num_topics:\n",
    "            self.load_model(topic_num)\n",
    "            print(f\"Processing Topic Number: {topic_num}\")  # Debug print\n",
    "            for i, text in enumerate(documents['Combined_Text']):  # Ensure you're iterating through the correct column\n",
    "                if self.documents.loc[i, 'Topic'] == topic_num:\n",
    "                    #print(f\"Processing Row: {i}\")  # Debug print\n",
    "                    encoding = self.tokenize_function(text)\n",
    "                    inputs = {\n",
    "                        'input_ids': encoding['input_ids'],\n",
    "                        'attention_mask': encoding['attention_mask']\n",
    "                    }\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        logits = outputs.logits\n",
    "                        if torch.any(torch.isnan(logits)):\n",
    "                            print(\"NaN detected in logits\")\n",
    "                        predicted_priority = logits.argmax(dim=1).cpu().numpy()[0]\n",
    "                        self.documents.at[i, \"Predicted_Priority\"] = predicted_priority\n",
    "        return self.documents\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9348bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Assessment\n",
    "class AccuracyAssessment:\n",
    "    def __init__(self, priority_levels):\n",
    "        self.actual_priority = None\n",
    "        self.predicted_priority = None\n",
    "        self.true_pos = 0\n",
    "        self.false_pos = 0\n",
    "        self.false_neg = 0\n",
    "        self.num_priority_levels = priority_levels\n",
    "        self.confusion_matrix = np.zeros((self.num_priority_levels, self.num_priority_levels), dtype=int)\n",
    "    \n",
    "    def update_vals(self, actual, predicted):\n",
    "        self.actual_priority = actual.astype(int)\n",
    "        self.predicted_priority = predicted.astype(int)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        for i in range(len(self.actual_priority)):\n",
    "            true_idx = self.actual_priority[i] - 1\n",
    "            pred_idx = self.predicted_priority[i] - 1\n",
    "            self.confusion_matrix[true_idx, pred_idx] += 1\n",
    "    \n",
    "    def calc_metrics(self, class_index):\n",
    "        self.true_pos = self.confusion_matrix[class_index, class_index]\n",
    "        self.false_pos = np.sum(self.confusion_matrix[:, class_index]) - self.true_pos\n",
    "        self.false_neg = np.sum(self.confusion_matrix[class_index, :]) - self.true_pos\n",
    "    \n",
    "    def precision(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_pos == 0:\n",
    "            return 0\n",
    "        precision = self.true_pos / (self.true_pos + self.false_pos)\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    def recall(self, class_index):\n",
    "        self.calc_metrics(class_index)\n",
    "        \n",
    "        if self.true_pos + self.false_neg == 0:\n",
    "            return 0\n",
    "        \n",
    "        recall = self.true_pos / (self.true_pos + self.false_neg)\n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    def fmeasure(self, class_index):\n",
    "        precision = self.precision(class_index)\n",
    "        recall = self.recall(class_index)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        \n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        return fmeasure\n",
    "    \n",
    "    def accuracyOverall(self):\n",
    "        accuratePriority = 0\n",
    "        for i in range(self.num_priority_levels):\n",
    "            self.calc_metrics(i)\n",
    "            accuratePriority += self.true_pos\n",
    "        accuratePriorities = accuratePriority / len(self.actual_priority)\n",
    "        return accuratePriorities\n",
    "    \n",
    "    def microAnalysis(self):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f_measures = []\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "            \n",
    "            precisions.append(precision_score)\n",
    "            recalls.append(recall_score)\n",
    "            f_measures.append(f_measure_score)\n",
    "        \n",
    "        micro_precision = sum(precisions) / sum(precisions + recalls)\n",
    "        micro_recall = sum(precisions) / sum(precisions + f_measures)\n",
    "        micro_fmeasure = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "        print(f\"Micro-Analysis for Priority Levels: Precision = {micro_precision:.4f}, Recall={micro_recall:.4f}, F-measure={micro_fmeasure:.4f}\")\n",
    "   \n",
    "    def macroAnalysis(self):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f_measures = []\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "            \n",
    "            precisions.append(precision_score)\n",
    "            recalls.append(recall_score)\n",
    "            f_measures.append(f_measure_score)\n",
    "\n",
    "        macro_precision = sum(precisions) / len(precisions)\n",
    "        macro_recall = sum(recalls) / len(recalls)\n",
    "        macro_fmeasure = sum(f_measures) / len(f_measures)\n",
    "        print(f\"Macro-Analysis for Priority Levels: Precision = {macro_precision:.4f}, Recall={macro_recall:.4f}, F-measure={macro_fmeasure:.4f}\")\n",
    "    def printAssessment(self):\n",
    "        for i in range(self.num_priority_levels):\n",
    "            precision_score = self.precision(i)\n",
    "            recall_score = self.recall(i)\n",
    "            f_measure_score = self.fmeasure(i)\n",
    "    \n",
    "            print(f\"Priority P{i+1}: Precision={precision_score:.4f}, Recall={recall_score:.4f}, F-measure={f_measure_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b514ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes together in pipeline:\n",
    "def priority_pipeline_without_assessment(data):\n",
    "    #PreProcessing\n",
    "    #preprocess = PreprocessingPineline(stop_words, punctuation)\n",
    "    #data = preprocess.data_to_tokens(data)\n",
    "    #Topic Modeling (Insert Trained Model Here, Save as a .pth)\n",
    "    lda_model_path = '/home/rpierson/PiersonREU/extracted/lda.pkl'\n",
    "    vectorizer_path = '/home/rpierson/PiersonREU/extracted/vec.pkl'\n",
    "    topic_model = LDATopicModelPipeline(lda_model_path, vectorizer_path)\n",
    "    topic_model.load_model()\n",
    "    df = pd.DataFrame(columns = ['Combined_Text', 'Topic'])\n",
    "    df['Combined_Text'] = data\n",
    "    df['Combined_Text'] = df['Combined_Text'].fillna(' ')\n",
    "    topic_distributions = topic_model.topic_distributions(df)\n",
    "    data = topic_model.append_topics(df, topic_distributions)\n",
    "    print(df)\n",
    "    vectorizer = topic_model.get_vectorizer()\n",
    "    \n",
    "    #Text Classification Per Topic (Insert Trained Model Here, Save as a .pth)\n",
    "    textClass = BertTextClassification()\n",
    "    data = textClass.priority(df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472affd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "to_see_results = pd.read_csv('/home/rpierson/PiersonREU/extracted/test_dataset_notpreprocessed.csv')\n",
    "to_see_results.head()\n",
    "\n",
    "to_see_results['Combined_Text'] = to_see_results['Title'] + \" \" + to_see_results['Component'] + \" \" + to_see_results['Description']\n",
    "\n",
    "actual = [to_see_results[\"Priority\"]]\n",
    "actual_df = pd.DataFrame(actual)\n",
    "actual_df = actual_df.transpose()\n",
    "actual_df\n",
    "\n",
    "Combined_Text = [to_see_results[\"Combined_Text\"]]\n",
    "df = pd.DataFrame(Combined_Text)\n",
    "df = df.transpose()\n",
    "df\n",
    "label_map = {'P1': 1, 'P2': 2, 'P3': 3, 'P4': 4, 'P5': 5}\n",
    "to_see_results['Priority'] = to_see_results['Priority'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77f0e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpierson/anaconda3/envs/secondenvi/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LatentDirichletAllocation from version 1.5.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/rpierson/anaconda3/envs/secondenvi/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.5.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Combined_Text  Topic\n",
      "0      Cant disable a feature Update  (deprecated - u...      9\n",
      "1      build id wrong in the about dialog UI In win32...      9\n",
      "2      [JFace] ConfigureColumnsDialog does not work c...      9\n",
      "3      Widget is disposed in ControlExample SWT - run...      4\n",
      "4      An internal error occurred during: Initializin...      4\n",
      "...                                                  ...    ...\n",
      "17027                                                         0\n",
      "17028  [GTK/Linux] Blank Windows with GTK3 UI I start...      9\n",
      "17029                                                         0\n",
      "17030  Crash (MacOS) - getIvar SWT Process:         e...      3\n",
      "17031                                                         0\n",
      "\n",
      "[17032 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpierson/anaconda3/envs/secondenvi/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rpierson/anaconda3/envs/secondenvi/lib/python3.11/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Topic Number: 9\n",
      "Processing Topic Number: 4\n",
      "Processing Topic Number: 0\n",
      "Processing Topic Number: 8\n",
      "Processing Topic Number: 7\n",
      "Processing Topic Number: 5\n",
      "Processing Topic Number: 6\n",
      "Processing Topic Number: 3\n",
      "Processing Topic Number: 2\n",
      "Processing Topic Number: 1\n"
     ]
    }
   ],
   "source": [
    "new_df = priority_pipeline_without_assessment(data = to_see_results['Combined_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a33c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0.0: 1, 1.0: 2, 2.0: 3, 3.0: 4, 4.0: 5}\n",
    "new_df['Predicted_Priority'] = new_df['Predicted_Priority'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88cd59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment = AccuracyAssessment(5)\n",
    "assessment.update_vals(to_see_results['Priority'], new_df[\"Predicted_Priority\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe540fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority P1: Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
      "Priority P2: Precision=0.0093, Recall=0.0033, F-measure=0.0048\n",
      "Priority P3: Precision=0.9747, Recall=0.9926, F-measure=0.9835\n",
      "Priority P4: Precision=0.8000, Recall=0.1509, F-measure=0.2540\n",
      "Priority P5: Precision=0.0000, Recall=0.0000, F-measure=0.0000\n"
     ]
    }
   ],
   "source": [
    "assessment.printAssessment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e274fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-Analysis for Priority Levels: Precision = 0.3568, Recall=0.2294, F-measure=0.2485\n"
     ]
    }
   ],
   "source": [
    "assessment.macroAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd6467bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-Analysis for Priority Levels: Precision = 0.6087, Recall=0.5895, F-measure=0.5989\n"
     ]
    }
   ],
   "source": [
    "assessment.microAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355974be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9676491310474401"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment.accuracyOverall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2653a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "extract_dir = '/home/rpierson/Files'\n",
    "file = os.path.join(extract_dir, 'BERTResults.csv')\n",
    "new_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b6945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
